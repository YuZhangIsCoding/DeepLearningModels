{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build LSTM with Keras\n",
    "\n",
    "In this notebook, I practised how to implement LSTM with Keras. I basically followed [this tutorial](https://adventuresinmachinelearning.com/keras-lstm-tutorial/), and its [corresponding code](https://github.com/adventuresinML/adventures-in-ml-code/blob/master/lstm_tutorial.py). It's easier for me to start with a high level API like Keras, and get farmiliar with the model, and then use more basic tensorflow blocks to build more customized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch and read data\n",
    "\n",
    "Same data used for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://mattmahoney.net/dc/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(filename, expected_bytes = None):\n",
    "    '''Download a file if not found'''\n",
    "    data_dir = os.path.join(os.getcwd(), 'data')\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    local_filename = os.path.join(data_dir, filename)\n",
    "    if not os.path.exists(local_filename):\n",
    "        local_filename, _ = urllib.request.urlretrieve(url+filename, local_filename)\n",
    "    \n",
    "    filesize = os.stat(local_filename).st_size\n",
    "    if expected_bytes and filesize == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print('Downloaded file', filename, 'with size of', filesize)\n",
    "        if expected_bytes:\n",
    "            raise Exception('Fail to verify'+local_filename)\n",
    "    return data_dir, local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "datapath, filename = fetch_data('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = read_data(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split them into training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = words[:(8*len(words))//10]\n",
    "valid_data = words[(8*len(words))//10:(9*len(words))//10]\n",
    "test_data = words[(9*len(words))//10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dictionary from training dataset\n",
    "\n",
    "Limit the size to top 10000 words only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dicts(words, num_words):\n",
    "    counts = [('UNK', -1)]\n",
    "    counts.extend(Counter(words).most_common(num_words-1))\n",
    "    word2ind = {}\n",
    "    for i, item in enumerate(counts):\n",
    "        word2ind[item[0]] = i\n",
    "    ind2word = dict(zip(word2ind.values(), word2ind.keys()))\n",
    "    return word2ind, ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2ind, ind2word = build_dicts(training_data, vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert words to indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_indexes(words, word2ind):\n",
    "    data = []\n",
    "    for word in words:\n",
    "        data.append(word2ind.get(word, word2ind['UNK']))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = words_to_indexes(training_data, word2ind)\n",
    "valid_data = words_to_indexes(valid_data, word2ind)\n",
    "test_data = words_to_indexes(test_data, word2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term']\n"
     ]
    }
   ],
   "source": [
    "print(words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term']\n"
     ]
    }
   ],
   "source": [
    "print([ind2word[item] for item in training_data[:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    def __init__(self, data, batch_size, num_lstm, vocabulary_size, skip_step=1):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.num_lstm = num_lstm\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.skip_step = skip_step\n",
    "        self.current_index = 0\n",
    "        \n",
    "    def generate(self):\n",
    "        x = np.ndarray(shape=(self.batch_size, self.num_lstm), dtype=np.int32)\n",
    "        y = np.ndarray(shape=(self.batch_size, self.num_lstm, vocabulary_size),\n",
    "                      dtype=np.int32)\n",
    "        while True:\n",
    "            if self.current_index+self.num_lstm >= len(self.data):\n",
    "                self.current_index = 0    \n",
    "            for i in range(self.batch_size):\n",
    "                x[i, :] = self.data[self.current_index: self.current_index+self.num_lstm]\n",
    "                _y = self.data[self.current_index+1: self.current_index+self.num_lstm+1]\n",
    "                y[i, :, :] = to_categorical(_y, num_classes=self.vocabulary_size)\n",
    "                self.current_index += self.skip_step\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lstm = 20\n",
    "batch_size = 32\n",
    "train_data_generator = BatchGenerator(training_data, batch_size, num_lstm, vocabulary_size,\n",
    "                                     skip_step=num_lstm)\n",
    "valid_data_generator = BatchGenerator(valid_data, batch_size, num_lstm, vocabulary_size,\n",
    "                                     skip_step=num_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LSTM cells with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 500\n",
    "use_dropout = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, hidden_size, input_length=num_lstm))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(vocabulary_size)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(),\n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 500)           5000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20, 500)           2002000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 20, 500)           2002000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 500)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 20, 10000)         5010000   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20, 10000)         0         \n",
      "=================================================================\n",
      "Total params: 14,014,000\n",
      "Trainable params: 14,014,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=datapath+'/model-{epoch:02d}.hdf5', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "21256/21256 [==============================] - 31819s 1s/step - loss: 5.9970 - categorical_accuracy: 0.1509 - val_loss: 5.7883 - val_categorical_accuracy: 0.1678\n",
      "\n",
      "Epoch 00001: saving model to /Users/yuzhang/ML/RNN/LSTM_Keras/data/model-01.hdf5\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "model.fit_generator(train_data_generator.generate(), \n",
    "                   len(training_data)//(batch_size*num_lstm),\n",
    "                   num_epochs,\n",
    "                   validation_data=valid_data_generator.generate(),\n",
    "                   validation_steps=len(valid_data)//(batch_size*num_lstm),\n",
    "                   callbacks=[checkpointer])\n",
    "model.save(datapath+'/final_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = next(valid_data_generator.generate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(temp[0])\n",
    "\n",
    "pred_indexes = np.argmax(prediction, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK UNK UNK UNK UNK UNK the to as UNK was UNK in the UNK gulf of after the UNK\n",
      "for UNK UNK was UNK in in in the UNK of the UNK UNK UNK UNK UNK UNK the one\n",
      "nine zero zero zero zero est UNK of been the UNK UNK minister of UNK UNK UNK UNK UNK UNK\n",
      "UNK the UNK to in UNK of the to be for the UNK to one one one UNK been UNK\n",
      "UNK UNK UNK of the UNK zero years were from in UNK council of UNK UNK council was the of\n"
     ]
    }
   ],
   "source": [
    "for item in pred_indexes[:5]:\n",
    "    print(' '.join(ind2word[_] for  _ in item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK al UNK al UNK from power such a vote is unusual in the arab countries shortly after the vote\n",
      "UNK UNK he was UNK only briefly after the death of UNK UNK al ahmed al UNK on january one\n",
      "five two zero zero six the cabinet has recommended the current prime minister UNK al ahmad al UNK al UNK\n",
      "to be elected UNK the parliament is expected to vote on his appointment in late january UNK has been the\n",
      "de facto ruler since the two previous UNK fell ill the national assembly the UNK national assembly or UNK al\n"
     ]
    }
   ],
   "source": [
    "true_indexes = np.argmax(temp[1], axis=2)\n",
    "for item in true_indexes[:5]:\n",
    "    print(' '.join(ind2word[_] for _ in item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given only 1 epoch of training, the model has limited ability to predict the correct next word! More training might give better results. Also attention could also be introduced into the model to yield better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
