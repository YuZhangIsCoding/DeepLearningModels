{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding with word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this note boook, I will show you how to implement vectorization of words using the Skip-Gram model. I am following Tensorflow's official [tutorial](https://www.tensorflow.org/tutorials/representation/word2vec). The original paper is by [Mikolov et al](https://arxiv.org/pdf/1301.3781.pdf). The following code is based on the [basic implementation of word2vec](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py) on github, and is reduced to only show the main body of the implementation. More advanced code can be found at [this link](https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://mattmahoney.net/dc/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(filename, expected_bytes = None):\n",
    "    '''Download a file if not found'''\n",
    "    data_dir = os.path.join(os.getcwd(), 'data')\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    local_filename = os.path.join(data_dir, filename)\n",
    "    if not os.path.exists(local_filename):\n",
    "        local_filename, _ = urllib.request.urlretrieve(url+filename, local_filename)\n",
    "    \n",
    "    filesize = os.stat(local_filename).st_size\n",
    "    if expected_bytes and filesize == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print('Downloaded file', filename, 'with size of', filesize)\n",
    "        if expected_bytes:\n",
    "            raise Exception('Fail to verify'+local_filename)\n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "filename = fetch_data('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = read_data(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'originated',\n",
       " 'as',\n",
       " 'a',\n",
       " 'term',\n",
       " 'of',\n",
       " 'abuse',\n",
       " 'first',\n",
       " 'used',\n",
       " 'against']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dictionary replace rare words with UNK token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dicts(words, n_words):\n",
    "    '''Build reference dictionaries'''\n",
    "    count = [['UNK', -1]] # why is it initialized as -1 instead of 0?\n",
    "    count.extend(collections.Counter(words).most_common(n_words-1))\n",
    "    word2ind = {}\n",
    "    for word, _ in count:\n",
    "        word2ind[word] = len(word2ind)\n",
    "    data = []\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = word2ind.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    ind2word = dict(zip(word2ind.values(), word2ind.keys()))\n",
    "    return data, count, word2ind, ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, count, word2ind, ind2word = build_dicts(vocabulary, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [ind2word[_] for _ in data[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training batches using skip-gram\n",
    "\n",
    "Given a center word, randomly select a word from its context window. Use this context word as training data, while the center word is the target, we can then build a batch of training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size%num_skips == 0 # make sure each skip has same number of dataset\n",
    "    assert num_skips < 2*skip_window+1\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2*skip_window+1 # [skip_window, target, skip_window]\n",
    "    buffer = collections.deque(maxlen=span) # create a queue to store window\n",
    "    if data_index+span > len(data):\n",
    "        data_index = 0 # reinitialize if current window exceeds the length\n",
    "    buffer.extend(data[data_index:data_index+span]) # append current window to the queue\n",
    "    data_index += span # number of window shifts is len(data)-span\n",
    "    # create a list of indexes for context words\n",
    "    context_words = [item for item in range(span) if item != skip_window]\n",
    "    for i in range(batch_size//num_skips):\n",
    "        words_from_window = random.sample(context_words, num_skips)\n",
    "        for j, target_word in enumerate(words_from_window):\n",
    "            # use center word as data\n",
    "            batch[i*num_skips+j] = buffer[skip_window]\n",
    "            # use random word from skip window as target\n",
    "            labels[i*num_skips+j, 0] = buffer[target_word] \n",
    "        if data_index == len(data):\n",
    "            # reinitialize if reaches to the end\n",
    "            buffer.extend(data[:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "    # backtrack to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index+len(data)-span)%len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glance at the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('originated', 'as'),\n",
       " ('originated', 'anarchism'),\n",
       " ('as', 'a'),\n",
       " ('as', 'originated'),\n",
       " ('a', 'term'),\n",
       " ('a', 'as'),\n",
       " ('term', 'a'),\n",
       " ('term', 'of')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "[(ind2word[i], ind2word[j[0]]) for i, j in zip(batch, labels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skip-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EMBEDDING_SIZE = 128 # dimention of the embedding vector\n",
    "SKIP_WINDOW = 1\n",
    "NUM_SKIPS = 2\n",
    "NUM_SAMPLED = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SIZE = 16\n",
    "VALID_WINDOW = 100 # only select the first 100 words to evaluate\n",
    "valid_examples = np.random.choice(VALID_WINDOW, VALID_SIZE, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model with tensorflow. For convinience, just build the essential parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[BATCH_SIZE, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an embedding and then look up word vector from this embedding\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, EMBEDDING_SIZE], -1.0, 1.0))\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NCE loss model](https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf) is a way to reduce the cost of softmax function by replacing it with logistic regression over much smaller sample size, which consists of a positive sample and several other negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the NCE loss model\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, EMBEDDING_SIZE],\n",
    "                                             stddev=1.0/np.sqrt(EMBEDDING_SIZE)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.nce_loss(\n",
    "        weights=nce_weights,\n",
    "        biases=nce_biases,\n",
    "        labels=train_labels,\n",
    "        inputs=embed,\n",
    "        num_sampled=NUM_SAMPLED,\n",
    "        num_classes=vocabulary_size\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "normalized_embeddings = embeddings/norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "                                        valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280.397\n",
      "195.874\n",
      "166.827\n",
      "168.633\n",
      "165.543\n",
      "95.4131\n",
      "124.41\n",
      "163.022\n",
      "161.284\n",
      "142.33\n",
      "79.6432\n",
      "53.8843\n",
      "153.529\n",
      "115.499\n",
      "82.0457\n",
      "73.0976\n",
      "62.2828\n",
      "67.9734\n",
      "86.612\n",
      "68.8936\n",
      "73.6611\n",
      "56.5932\n",
      "66.9391\n",
      "98.8964\n",
      "49.5145\n",
      "68.8724\n",
      "39.0783\n",
      "53.9223\n",
      "31.3299\n",
      "62.6922\n",
      "35.2591\n",
      "32.3451\n",
      "47.4218\n",
      "32.6992\n",
      "30.9951\n",
      "63.1502\n",
      "51.3414\n",
      "47.8295\n",
      "36.6661\n",
      "26.3181\n",
      "34.6017\n",
      "35.4397\n",
      "37.6986\n",
      "73.3687\n",
      "51.6947\n",
      "32.6635\n",
      "26.1504\n",
      "39.9885\n",
      "40.0424\n",
      "11.9627\n",
      "27.6031\n",
      "46.4044\n",
      "43.1606\n",
      "7.60296\n",
      "25.0566\n",
      "19.238\n",
      "42.1086\n",
      "28.7141\n",
      "52.1673\n",
      "24.5821\n",
      "23.0984\n",
      "35.3962\n",
      "58.4864\n",
      "35.8538\n",
      "24.8007\n",
      "60.674\n",
      "46.226\n",
      "41.0131\n",
      "29.0721\n",
      "34.6343\n",
      "20.4665\n",
      "21.5675\n",
      "16.381\n",
      "39.0353\n",
      "25.8767\n",
      "27.4252\n",
      "17.5337\n",
      "20.7826\n",
      "14.6143\n",
      "33.0125\n",
      "39.725\n",
      "31.9468\n",
      "17.5575\n",
      "43.666\n",
      "20.301\n",
      "7.0637\n",
      "30.215\n",
      "18.5779\n",
      "14.3694\n",
      "21.5498\n",
      "8.37587\n",
      "16.0859\n",
      "18.2649\n",
      "15.1885\n",
      "14.9982\n",
      "17.8057\n",
      "13.0541\n",
      "18.8006\n",
      "16.3858\n",
      "14.6411\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(BATCH_SIZE, NUM_SKIPS,\n",
    "                                                   SKIP_WINDOW)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "        _, loss_train = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        if step%100 == 0:\n",
    "            print(loss_train)\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism and its corresponding vector:\n",
      " [ 0.09323777 -0.01108901  0.03200433 -0.10120311  0.13836347 -0.0466122\n",
      "  0.02027027 -0.0148513  -0.01205099 -0.07231995 -0.06766896 -0.00947833\n",
      "  0.16027215 -0.0833019   0.09467531 -0.05385627  0.03214695 -0.01431454\n",
      " -0.07629175 -0.04613779  0.07980682 -0.0216409  -0.05562487  0.08287641\n",
      " -0.0306728  -0.02641387  0.05044418  0.03224257  0.00646495  0.10593925\n",
      " -0.15757279 -0.10095137  0.13243385  0.09088251  0.16086781 -0.01930537\n",
      "  0.114754   -0.09769886  0.10203437 -0.08148076  0.05227184  0.09976345\n",
      "  0.00686788 -0.14601114  0.05952242 -0.08311615 -0.07713651  0.03563159\n",
      " -0.02959167  0.07112483  0.16031735 -0.00664568 -0.11878739  0.11108568\n",
      " -0.05220035  0.10424857 -0.13774247 -0.1038047   0.0459159  -0.04485492\n",
      "  0.10481818 -0.15517806  0.03311496  0.12910315 -0.09689718  0.08632062\n",
      " -0.08316438  0.06241824 -0.04387079 -0.07843991  0.0839075  -0.05629348\n",
      "  0.12754366 -0.06455048 -0.01348033 -0.09118778 -0.06397358  0.08144351\n",
      " -0.07626761 -0.0920784  -0.13531934  0.02274129  0.11627972  0.03221974\n",
      " -0.00102668 -0.12191085 -0.01137103  0.14031969  0.14655674 -0.08579203\n",
      " -0.07053871 -0.12068859 -0.05002748 -0.12467075 -0.01326944 -0.13709873\n",
      " -0.14722434 -0.06510075  0.11777068  0.09771171 -0.01469816 -0.13360235\n",
      " -0.04655845  0.10981151  0.07984615  0.03950593 -0.03942723  0.05460652\n",
      "  0.08377048 -0.12785618 -0.09250577 -0.10082985  0.02639473  0.14933152\n",
      "  0.15286364 -0.09568779 -0.03869141 -0.03581347  0.07149805  0.08771108\n",
      " -0.06484973 -0.05971221 -0.1147771   0.1315445  -0.12188491 -0.0582869\n",
      " -0.07214997 -0.04029901]\n"
     ]
    }
   ],
   "source": [
    "print(ind2word[data[0]], 'and its corresponding vector:\\n', final_embeddings[data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
