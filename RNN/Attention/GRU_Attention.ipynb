{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation by RNN with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This note book shows a sequence to sequence (seq2seq) implementaion using tensorflow and keras. Basically I followed [this notebook](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb#scrollTo=CiwtNgENbx2g)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://download.tensorflow.org/data/spa-eng.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(filename):\n",
    "    data_dir = os.path.join(os.getcwd(), 'data')\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.mkdir(data_dir)\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        filepath, _ = urllib.request.urlretrieve(url, filepath)\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = fetch_data('spa-eng.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    '''Convert the unicode file to ascii'''\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    '''Preprocess sentence.\n",
    "    Replace everything with space except for some specific punctuations.\n",
    "    '''\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    \n",
    "    # create a space between a word and the punctuation\n",
    "    w = re.sub(r'([?.!,¿])', r' \\1 ', w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r'[^a-zA-Z?.!,¿]+', ' ', w)\n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # adding a start and and end token to the setence\n",
    "    w = '<SOS> '+w+' <EOS>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath, num_samples):\n",
    "    with zipfile.ZipFile(filepath) as f:\n",
    "        lines = tf.compat.as_str(f.read(f.namelist()[-1])).split('\\n')\n",
    "    word_pairs = [[preprocess_sentence(w) \n",
    "                   for w in l.split('\\t')] for l in lines[:num_samples]]\n",
    "    return word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(filepath, 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<SOS> go . <EOS>', '<SOS> ve . <EOS>'],\n",
       " ['<SOS> go . <EOS>', '<SOS> vete . <EOS>'],\n",
       " ['<SOS> go . <EOS>', '<SOS> vaya . <EOS>'],\n",
       " ['<SOS> go . <EOS>', '<SOS> vayase . <EOS>'],\n",
       " ['<SOS> hi . <EOS>', '<SOS> hola . <EOS>'],\n",
       " ['<SOS> run ! <EOS>', '<SOS> corre ! <EOS>'],\n",
       " ['<SOS> run . <EOS>', '<SOS> corred . <EOS>'],\n",
       " ['<SOS> who ? <EOS>', '<SOS> ¿ quien ? <EOS>'],\n",
       " ['<SOS> fire ! <EOS>', '<SOS> fuego ! <EOS>'],\n",
       " ['<SOS> fire ! <EOS>', '<SOS> incendio ! <EOS>']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<SOS> we will have much fun . <EOS>',\n",
       "  '<SOS> vamos a divertirnos mucho . <EOS>'],\n",
       " ['<SOS> we will not surrender . <EOS>', '<SOS> no nos rendiremos . <EOS>'],\n",
       " ['<SOS> we wish you well , tom . <EOS>',\n",
       "  '<SOS> te deseamos lo mejor , tom . <EOS>'],\n",
       " ['<SOS> we won t let you down . <EOS>',\n",
       "  '<SOS> no te decepcionaremos . <EOS>'],\n",
       " ['<SOS> we work to earn money . <EOS>',\n",
       "  '<SOS> trabajamos para ganar dinero . <EOS>'],\n",
       " ['<SOS> we ll go after we eat . <EOS>',\n",
       "  '<SOS> iremos despues de comer . <EOS>'],\n",
       " ['<SOS> we ll live like kings . <EOS>', '<SOS> viviremos como reyes . <EOS>'],\n",
       " ['<SOS> we ll lose everything . <EOS>', '<SOS> lo perderemos todo . <EOS>'],\n",
       " ['<SOS> we ll meet right here . <EOS>',\n",
       "  '<SOS> nos encontraremos aqui mismo . <EOS>'],\n",
       " ['<SOS> we ll see you at . <EOS>',\n",
       "  '<SOS> te veremos a las dos y media . <EOS>']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dictionaries\n",
    "\n",
    "Since we need look up tables for both spanish and english, we gonna build a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordIndex(object):\n",
    "    def __init__(self, sentence_iter):\n",
    "        '''\n",
    "        sentence_iter: an iterable object that gives an sentence at each iteration\n",
    "        '''\n",
    "        self.sentence_iter = sentence_iter\n",
    "        # do not include 'UNK' for know\n",
    "        # vocab = ['<pad>', '<UNK>']\n",
    "        self.vocab = ['<pad>']\n",
    "        self.build_dict()\n",
    "        \n",
    "    def build_dict(self):\n",
    "        seen = set()\n",
    "        for setence in self.sentence_iter:\n",
    "            seen.update(setence.split())\n",
    "        self.vocab.extend(sorted(seen))\n",
    "        \n",
    "        self.word2ind = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.ind2word = {i: word for i, word in enumerate(self.vocab)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.word2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ind = WordIndex(en for en, sp in data)\n",
    "target_ind = WordIndex(sp for en, sp in data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert words to indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = [[input_ind.word2ind[word] for word in en.split()] for en, sp in data]\n",
    "target_tensor = [[target_ind.word2ind[word] for word in sp.split()] for en, sp in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_input = max([len(item) for item in input_tensor])\n",
    "max_len_target = max([len(item) for item in target_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the tensor to have the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = keras.preprocessing.sequence.pad_sequences(input_tensor,\n",
    "                                                         maxlen=max_len_input,\n",
    "                                                         padding='post')\n",
    "target_tensor = keras.preprocessing.sequence.pad_sequences(target_tensor,\n",
    "                                                          maxlen=max_len_target,\n",
    "                                                          padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create mini-batch\n",
    "\n",
    "Note that this won't work under eager mode (tensorflow 1.5 version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(train, target, batch_size=32, epochs=1, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train, target))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(len(train))\n",
    "    dataset = dataset.batch(batch_size).repeat(epochs)\n",
    "    train_batch, target_batch = dataset.make_one_shot_iterator().get_next()\n",
    "    return train_batch, target_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "\n",
    "We gonna build encoder and decoder based on keras.Model class. The RNN we gonna use is GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = keras.layers.GRU(self.hidden_size, return_sequences=True,\n",
    "                                   return_state=True, recurrent_activation='sigmoid',\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "    def __call__(self, x, hidden_state):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden_state)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented attention model when decode. I am following Bahdanau's additive style, where the score is the product of a weighting matrix and the activation of encoder output and hidden states. Summing up encoder output weighted by the attention score, we obtain the context vector. And by concating this context vector with embedding output for decoder, we send the merged vector to GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = keras.layers.GRU(self.hidden_size, return_sequences=True,\n",
    "                                   return_state=True, recurrent_activation='sigmoid',\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # attention\n",
    "        self.W1 = keras.layers.Dense(self.hidden_size)\n",
    "        self.W2 = keras.layers.Dense(self.hidden_size)\n",
    "        self.V = keras.layers.Dense(1)\n",
    "        \n",
    "    def __call__(self, x, hidden_state, encoder_output):\n",
    "        '''encoder_output shape == (batch_size, max_length, hidden_size)\n",
    "        hidden_state shape == (batch_size, hidden_size)\n",
    "        hidden_with_time_axis shape = (batch_size, 1, hidden_size)\n",
    "        perform addition to calculate the attention score.\n",
    "        '''\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden_state, 1)\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(encoder_output)+self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape == (batch_size, hidden_size)\n",
    "        context_vector = tf.reduce_sum(attention_weights*encoder_output, axis=1)\n",
    "        \n",
    "        # x shape == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        # concat input embedding with summed attention\n",
    "        x = tf.concat([tf.expand_dims(context_vector, axis=1), x], axis = -1)\n",
    "        \n",
    "        output, state = self.gru(x)#, initial_state=hidden_state)\n",
    "        output = tf.reshape(output, (-1, output.shape[-1]))\n",
    "        \n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights\n",
    "    \n",
    "    def inilialize_hidden_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, logits):\n",
    "    '''mask padded words'''\n",
    "    mask = 1-np.equal(real, 0)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=logits)*mask\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "hidden_size = 512\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ind.word2ind['<SOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "inputs = tf.placeholder(tf.int32, shape=[None, max_len_input])\n",
    "labels = tf.placeholder(tf.int32, shape=[None, max_len_target])\n",
    "\n",
    "encoder = Encoder(len(input_ind), embedding_dim, hidden_size)\n",
    "decoder = Decoder(len(target_ind), embedding_dim, hidden_size)\n",
    "\n",
    "hidden_state = encoder.initialize_hidden_state(tf.shape(inputs)[0])\n",
    "enc_output, enc_hidden = encoder(inputs, hidden_state)\n",
    "dec_hidden = enc_hidden\n",
    "\n",
    "dec_input = tf.fill((tf.shape(inputs)[0], 1), target_ind.word2ind['<SOS>'])\n",
    "loss = tf.get_variable('marginalLoss', [], dtype=tf.float32, \n",
    "                       initializer=tf.constant_initializer(0), trainable=False)\n",
    "\n",
    "# training loss\n",
    "for t in range(1, max_len_target):\n",
    "    logits, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "    loss = loss+loss_function(labels[:, t], logits)\n",
    "    dec_input = tf.expand_dims(labels[:, t], 1)\n",
    "    \n",
    "# prediction\n",
    "dec_input = tf.fill((tf.shape(inputs)[0], 1), target_ind.word2ind['<SOS>'])\n",
    "dec_hidden = enc_hidden\n",
    "# results = tf.zeros((tf.shape(inputs)[0], max_len_target), tf.int32)\n",
    "out_list = []\n",
    "for t in range(max_len_target):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_output)\n",
    "    predicted_id = tf.argmax(predictions, axis=1)\n",
    "    out_list.append(predicted_id)\n",
    "    dec_input = tf.expand_dims(predicted_id, 1)\n",
    "predicts = tf.transpose(tf.stack(out_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_batch, y_batch = get_batch(input_tensor, target_tensor, batch_size=batch_size, epochs=1)\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     x, y = sess.run([x_batch, y_batch])\n",
    "#     feed_dict = {inputs: x, labels: y}\n",
    "#     temp = sess.run(predicts, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "max_iter = 500000\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "x_batch, y_batch = get_batch(input_tensor, target_tensor, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss: 137.240677\n",
      "Iteration 100, loss: 29.680830\n",
      "Iteration 200, loss: 26.125179\n",
      "Iteration 300, loss: 26.297970\n",
      "Iteration 400, loss: 23.713600\n",
      "Iteration 500, loss: 23.031403\n",
      "Iteration 600, loss: 24.013575\n",
      "Iteration 700, loss: 23.377451\n",
      "Iteration 800, loss: 21.455414\n",
      "Iteration 900, loss: 20.955591\n",
      "Iteration 1000, loss: 20.509119\n",
      "Iteration 1100, loss: 19.184927\n",
      "Iteration 1200, loss: 19.717228\n",
      "Iteration 1300, loss: 18.034071\n",
      "Iteration 1400, loss: 18.910034\n",
      "Iteration 1500, loss: 15.334837\n",
      "Iteration 1600, loss: 18.521866\n",
      "Iteration 1700, loss: 18.076632\n",
      "Iteration 1800, loss: 17.313713\n",
      "Iteration 1900, loss: 14.627246\n",
      "Iteration 2000, loss: 16.352417\n",
      "Iteration 2100, loss: 14.876765\n",
      "Iteration 2200, loss: 13.496007\n",
      "Iteration 2300, loss: 13.763338\n",
      "Iteration 2400, loss: 13.385315\n",
      "Iteration 2500, loss: 11.982315\n",
      "Iteration 2600, loss: 13.110905\n",
      "Iteration 2700, loss: 14.675159\n",
      "Iteration 2800, loss: 12.254469\n",
      "Iteration 2900, loss: 11.638268\n",
      "Iteration 3000, loss: 9.245835\n",
      "Iteration 3100, loss: 11.189074\n",
      "Iteration 3200, loss: 10.319814\n",
      "Iteration 3300, loss: 8.603122\n",
      "Iteration 3400, loss: 9.091704\n",
      "Iteration 3500, loss: 10.149076\n",
      "Iteration 3600, loss: 8.611110\n",
      "Iteration 3700, loss: 8.104351\n",
      "Iteration 3800, loss: 7.324904\n",
      "Iteration 3900, loss: 7.782757\n",
      "Iteration 4000, loss: 6.920548\n",
      "Iteration 4100, loss: 7.903048\n",
      "Iteration 4200, loss: 6.302925\n",
      "Iteration 4300, loss: 6.037416\n",
      "Iteration 4400, loss: 6.493895\n",
      "Iteration 4500, loss: 6.491760\n",
      "Iteration 4600, loss: 5.832527\n",
      "--- Read all epochs ---\n",
      "--- Training Ends ---\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    n_iter = 0\n",
    "    try:\n",
    "        while n_iter < max_iter:\n",
    "            x, y = sess.run([x_batch, y_batch])\n",
    "            feed_dict = {inputs: x, labels: y}\n",
    "            _, _loss = sess.run([optimizer, loss], feed_dict)\n",
    "            if n_iter %100 == 0:\n",
    "                print('Iteration %d, loss: %f' %(n_iter, _loss))\n",
    "            n_iter += 1\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('--- Read all epochs ---')\n",
    "    print('--- Training Ends ---')\n",
    "    saver.save(sess, 'checkpoints/attention.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/attention.ckpt\n"
     ]
    }
   ],
   "source": [
    "X_batch, _ = get_batch(input_tensor, target_tensor, batch_size=batch_size, shuffle=False)\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    x = sess.run(X_batch)\n",
    "    feed_dict = {inputs: x}\n",
    "    out = sess.run(predicts, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9090,    3,    4, ...,    0,    0,    0],\n",
       "       [9090,    3,    4, ...,    0,    0,    0],\n",
       "       [9090,    3,    4, ...,    0,    0,    0],\n",
       "       ..., \n",
       "       [6211, 5498, 9071, ...,    0,    0,    0],\n",
       "       [6211, 5498, 9071, ...,    0,    0,    0],\n",
       "       [6211, 5498, 9071, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ve . <EOS>',\n",
       "  've . <EOS>',\n",
       "  've . <EOS>',\n",
       "  've . <EOS>',\n",
       "  'hola . <EOS>',\n",
       "  'corre ! <EOS>',\n",
       "  'corre . <EOS>',\n",
       "  '¿ quien es ? <EOS>',\n",
       "  'fuego ! <EOS>',\n",
       "  'fuego ! <EOS>',\n",
       "  'fuego ! <EOS>',\n",
       "  'ayuda ! <EOS>',\n",
       "  'ayuda ! <EOS>',\n",
       "  'ayuda ! <EOS>',\n",
       "  'coge ! <EOS>',\n",
       "  'coge . <EOS>',\n",
       "  'dejen de nuevo . <EOS>',\n",
       "  'dejen de nuevo . <EOS>',\n",
       "  'dejen de nuevo . <EOS>',\n",
       "  'espera ! <EOS>',\n",
       "  'espera . <EOS>',\n",
       "  'entra . <EOS>',\n",
       "  'entra . <EOS>',\n",
       "  'hola ! <EOS>',\n",
       "  'me quede . <EOS>',\n",
       "  'me quede . <EOS>',\n",
       "  'yo lo entiendo . <EOS>',\n",
       "  'yo mismo ! <EOS>',\n",
       "  'oh , no ! <EOS>',\n",
       "  'tomatelo con soda . <EOS>',\n",
       "  'sonrie . <EOS>',\n",
       "  'se quedaron ! <EOS>',\n",
       "  'se quedaron ! <EOS>',\n",
       "  'sal de otro lado . <EOS>',\n",
       "  'vete ahora mismo . <EOS>',\n",
       "  'simplifica ! <EOS>',\n",
       "  '¿ entendiste ? <EOS>',\n",
       "  '¿ entendiste ? <EOS>',\n",
       "  'el se cayo . <EOS>',\n",
       "  'subete a tiempo . <EOS>',\n",
       "  'abrazame . <EOS>',\n",
       "  'me quede . <EOS>',\n",
       "  'se que se que se . <EOS>',\n",
       "  'me dejaron . <EOS>',\n",
       "  'me acuerdo de menos . <EOS>',\n",
       "  'perdi la consciencia . <EOS>',\n",
       "  'dimito . <EOS>',\n",
       "  'dimito . <EOS>',\n",
       "  'me siento . <EOS>',\n",
       "  'estoy divorciada . <EOS>',\n",
       "  'estoy levantado . <EOS>',\n",
       "  'escucha . <EOS>',\n",
       "  'escucha . <EOS>',\n",
       "  'escucha . <EOS>',\n",
       "  'no lo vamos a seguridad ! <EOS>',\n",
       "  'no lo vamos a seguridad ! <EOS>',\n",
       "  'no lo vamos a seguridad ! <EOS>',\n",
       "  'no lo vamos a seguridad ! <EOS>',\n",
       "  'no lo vamos a seguridad ! <EOS>',\n",
       "  'no lo vamos a seguridad ! <EOS>',\n",
       "  'no lo vamos a seguridad ! <EOS>',\n",
       "  'no lo vamos a seguridad ! <EOS>',\n",
       "  'no lo vamos a seguridad ! <EOS>',\n",
       "  'no lo vamos a seguridad ! <EOS>']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[' '.join(target_ind.ind2word[item] for item in temp if item != 0) for temp in out]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<SOS> ve . <EOS>',\n",
       "  '<SOS> vete . <EOS>',\n",
       "  '<SOS> vaya . <EOS>',\n",
       "  '<SOS> vayase . <EOS>',\n",
       "  '<SOS> hola . <EOS>',\n",
       "  '<SOS> corre ! <EOS>',\n",
       "  '<SOS> corred . <EOS>',\n",
       "  '<SOS> ¿ quien ? <EOS>',\n",
       "  '<SOS> fuego ! <EOS>',\n",
       "  '<SOS> incendio ! <EOS>',\n",
       "  '<SOS> disparad ! <EOS>',\n",
       "  '<SOS> ayuda ! <EOS>',\n",
       "  '<SOS> socorro ! auxilio ! <EOS>',\n",
       "  '<SOS> auxilio ! <EOS>',\n",
       "  '<SOS> salta ! <EOS>',\n",
       "  '<SOS> salte . <EOS>',\n",
       "  '<SOS> parad ! <EOS>',\n",
       "  '<SOS> para ! <EOS>',\n",
       "  '<SOS> pare ! <EOS>',\n",
       "  '<SOS> espera ! <EOS>',\n",
       "  '<SOS> esperen . <EOS>',\n",
       "  '<SOS> continua . <EOS>',\n",
       "  '<SOS> continue . <EOS>',\n",
       "  '<SOS> hola . <EOS>',\n",
       "  '<SOS> corri . <EOS>',\n",
       "  '<SOS> corria . <EOS>',\n",
       "  '<SOS> lo intento . <EOS>',\n",
       "  '<SOS> he ganado ! <EOS>',\n",
       "  '<SOS> oh , no ! <EOS>',\n",
       "  '<SOS> tomatelo con soda . <EOS>',\n",
       "  '<SOS> sonrie . <EOS>',\n",
       "  '<SOS> al ataque ! <EOS>',\n",
       "  '<SOS> atacad ! <EOS>',\n",
       "  '<SOS> levanta . <EOS>',\n",
       "  '<SOS> ve ahora mismo . <EOS>',\n",
       "  '<SOS> lo tengo ! <EOS>',\n",
       "  '<SOS> ¿ lo pillas ? <EOS>',\n",
       "  '<SOS> ¿ entendiste ? <EOS>',\n",
       "  '<SOS> el corrio . <EOS>',\n",
       "  '<SOS> metete adentro . <EOS>',\n",
       "  '<SOS> abrazame . <EOS>',\n",
       "  '<SOS> me cai . <EOS>',\n",
       "  '<SOS> yo lo se . <EOS>',\n",
       "  '<SOS> sali . <EOS>',\n",
       "  '<SOS> menti . <EOS>',\n",
       "  '<SOS> perdi . <EOS>',\n",
       "  '<SOS> dimito . <EOS>',\n",
       "  '<SOS> renuncie . <EOS>',\n",
       "  '<SOS> estoy trabajando . <EOS>',\n",
       "  '<SOS> tengo diecinueve . <EOS>',\n",
       "  '<SOS> estoy levantado . <EOS>',\n",
       "  '<SOS> escucha . <EOS>',\n",
       "  '<SOS> escuche . <EOS>',\n",
       "  '<SOS> escuchen . <EOS>',\n",
       "  '<SOS> no puede ser ! <EOS>',\n",
       "  '<SOS> de ninguna manera . <EOS>',\n",
       "  '<SOS> de ninguna manera ! <EOS>',\n",
       "  '<SOS> imposible ! <EOS>',\n",
       "  '<SOS> de ningun modo ! <EOS>',\n",
       "  '<SOS> de eso nada ! <EOS>',\n",
       "  '<SOS> ni cagando ! <EOS>',\n",
       "  '<SOS> mangos ! <EOS>',\n",
       "  '<SOS> minga ! <EOS>',\n",
       "  '<SOS> ni en pedo ! <EOS>']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[' '.join(target_ind.ind2word[item] for item in temp if item != 0) \n",
    "                      for temp in target_tensor[:batch_size]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/attention.ckpt\n"
     ]
    }
   ],
   "source": [
    "X_batch, _ = get_batch(input_tensor, target_tensor, batch_size=batch_size, shuffle=False)\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    x = sess.run(X_batch)\n",
    "    feed_dict = {inputs: x}\n",
    "    temp = sess.run(attention_weights, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention of input words (11 words) for the last word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.71082669],\n",
       "       [ 0.0059496 ],\n",
       "       [ 0.2104039 ],\n",
       "       [ 0.04182935],\n",
       "       [ 0.01627783],\n",
       "       [ 0.0026149 ],\n",
       "       [ 0.00209989],\n",
       "       [ 0.00236544],\n",
       "       [ 0.00249651],\n",
       "       [ 0.00255246],\n",
       "       [ 0.00258339]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
